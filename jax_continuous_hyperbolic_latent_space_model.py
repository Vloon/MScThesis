# -*- coding: utf-8 -*-
"""JAX Continuous Hyperbolic Latent Space Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1Q-R7yZ1JsmY_ad-Yk2esRHYRc2H_ia
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install blackjax==0.9.5
# # !pip install blackjax-nightly

# Basics
import matplotlib.pyplot as plt
from matplotlib.patches import Arc, Ellipse
import matplotlib.colors as mcolors
import numpy as np
import networkx as nx
import time

# File stuff
from google.colab import files
import pickle

# SVM
from sklearn import svm

# Sampling
import jax
import jax.numpy as jnp
from jax.config import config
import jax.scipy as jsp
import jax.scipy.stats as jstats
import blackjax as bjx
import blackjax.smc.resampling as resampling
config.update("jax_enable_x64", True)

# Typings
from jax._src.prng import PRNGKeyArray
from jax._src.typing import ArrayLike
from blackjax.types import PyTree
from blackjax.mcmc.rmh import RMHState
from matplotlib import axes as Axes # I want types to be capitalized for some reason.
from typing import Callable

print(bjx.__version__)

"""## Utility functions"""

def plot_hyperbolic_edges(p:ArrayLike, A:ArrayLike, ax:Axes, R:float=1, linewidth:float=0.5, threshold:float=0.4, continuous:bool=False, bkst:bool=False) -> Axes:
    """
    PARAMS
    p (N,2) : points on the Poincaré disk
    A (N,2) or (N*(N-1)/2) : (upper triangle of the) adjacency matrix.
    ax : axis to be plotted on
    R : disk radius
    linwidth : Linewidth of the edges
    """
    def mirror(p:ArrayLike, R:float=1) -> ArrayLike:
        """
        Mirrors point p in circle with R = 1
        Based on: https://math.stackexchange.com/questions/1322444/how-to-construct-a-line-on-a-poincare-disk
        INPUT:
            p (N,2) : N points on a 2-dimensional Poincaré disk
        RETURNS:
            p_inv (N,2) : Mirror of p
        """
        N, D = p.shape
        p_norm = np.sum(p**2,1)
        p_inv = R**2*p/np.reshape(np.repeat(p_norm,D), newshape=(N,D))
        return p_inv

    def bisectors(p:ArrayLike, R:float=1) -> tuple:
        """
        Returns the function for the perpendicular bisector as ax + b
        Based on: https://www.allmath.com/perpendicularbisector.php
        INPUT:
            p (N,2) : List of points
            R : Size of the disk
        RETURNS:
            a_self (N*(N+1)/2) : Slopes of the bisectors for each combination of points in p
            b_self (N*(N+1)/2) : Intersects of the bisectors for each combination of points in p
            a_inv (N) : Slopes of the bisectors for each point with its mirror
            b_inv (N) : Intersects of the bisectors for each point with its mirror
        """
        N, D = p.shape
        assert D == 2, 'Cannot visualize a Poincaré disk with anything other than 2 dimensional points'
        triu_indices = np.triu_indices(N, k=1) # No self-connections

        # Get mirror of points
        p_inv = mirror(p, R)

        # Tile to get all combinations
        x_rep = np.tile(p[:,0], N).reshape((N,N))
        y_rep = np.tile(p[:,1], N).reshape((N,N))

        # Get midpoints
        mid_x_self = ((x_rep.T+x_rep)/2)[triu_indices]
        mid_y_self = ((y_rep.T+y_rep)/2)[triu_indices]
        mid_x_inv  = (p[:,0]+p_inv[:,0])/2
        mid_y_inv  = (p[:,1]+p_inv[:,1])/2

        # Get slopes
        dx_self = (x_rep - x_rep.T)[triu_indices]
        dy_self = (y_rep- y_rep.T)[triu_indices]
        dx_inv  = p[:,0] - p_inv[:,0]
        dy_inv  = p[:,1] - p_inv[:,1]

        a_self = -1/(dy_self/dx_self)
        a_inv  = -1/(dy_inv/dx_inv)

        # Get intersects
        b_self = -a_self*mid_x_self + mid_y_self
        b_inv  = -a_inv*mid_x_inv + mid_y_inv

        return a_self, b_self, a_inv, b_inv

    N, D = p.shape
    assert D == 2, 'Cannot visualize a Poincaré disk with anything other than 2 dimensional points'
    if len(A.shape) == 2: # Get the upper triangle
        A = A[np.triu_indices(N, k=1)]

    ## Calculate perpendicular bisectors for points in p with each other, and with their mirrors.
    a_self, b_self, a_inv, b_inv  = bisectors(p,R)

    # Repeat elements according to repetitions in triangle number
    first_triu_idc = np.triu_indices(N,k=1)[0]
    a_inv_rep = np.array([a_inv[i] for i in first_triu_idc])
    b_inv_rep = np.array([b_inv[i] for i in first_triu_idc])
    px_rep = np.array([p[i,0] for i in first_triu_idc])
    py_rep = np.array([p[i,1] for i in first_triu_idc])

    # Get coordinates and radius of midpoint of the circle
    cx = (b_self-b_inv_rep)/(a_inv_rep-a_self)
    cy = a_self*cx + b_self
    cR = np.sqrt((px_rep-cx)**2 + (py_rep-cy)**2)

    second_triu_idc = np.triu_indices(N,k=1)[1]
    qx_rep = np.array([p[i,0] for i in second_triu_idc])
    qy_rep = np.array([p[i,1] for i in second_triu_idc])

    theta_p = np.degrees(np.arctan2(py_rep-cy, px_rep-cx))
    theta_q = np.degrees(np.arctan2(qy_rep-cy, qx_rep-cx))

    for i in range(N*(N-1)//2):
        if A[i] >= threshold:
            # Honestly... can't really tell you why this works but it does so someone else can do the math.
            if cx[i] > 0:
                theta_p[i] = theta_p[i]%360
                theta_q[i] = theta_q[i]%360

            # Draw arc
            arc = Arc(xy=(cx[i], cy[i]), width=2*cR[i], height=2*cR[i], angle=0, theta1=min(theta_p[i],theta_q[i]), theta2=max(theta_p[i],theta_q[i]), linewidth=linewidth, alpha=A[i])
            ax.add_patch(arc)

    return ax

def plot_network(network:nx.Graph,
                 pos:dict=None,
                 ax:Axes=None,
                 title:str=None,
                 node_color:list=None,
                 node_size:list=None,
                 edge_width:float=0.5,
                 disk_radius:float=1.,
                 hyperbolic:bool=False,
                 continuous:bool=False,
                 bkst:bool=False,
                 threshold:float=0.4,
                 margin:float=0.1) -> Axes:
    """
    Plots a network with the given positions.
    PARAMS:
    network : networkx object containing edge weights
    pos : position dictionary, with keys 1:N_nodes. defaults to spring layout
    ax : axis to plot the network in
    title : title to display
    node_color : list of valid plt colors for the nodes. defaults to n*['k']
    node_size : list of node sizes. defaults to scale with degree.
    edge_width : width of the edges
    disk_radius : the radius of the size of the plot
    hyperbolic : whether the network should be plotted in hyperbolic space or Euclidean space
    continuous : whether the edge weights are continuous or binary
    bkst : whether to deal with the first two nodes as Bookstein nodes
    threshold : minimum edge weight for the edge to be plotted
    margin : percentage of disk radius to be added as whitespace
    """
    if (pos is None and nx.get_node_attributes(network, 'pos') is None) or (pos is None and len(nx.get_node_attributes(network, 'pos'))==0):
        pos = nx.spring_layout(network)
    elif pos is None:
        pos = nx.get_node_attributes(network, 'pos')
    n = len(network.nodes())
    if node_color is None:
        node_color = n*['k']
    clean_ax = True
    if ax is None:
        plt.figure(facecolor='w')
        ax = plt.gca()
        clean_ax = False
    A = nx.convert_matrix.to_numpy_array(network)
    if node_size is None:
        if continuous:
            d = np.sum(A,axis=0)
            node_size = [v**2/10 for v in d]
        else:
            d = dict(nx.degree(network))
            node_size = [v**2/10 for v in d.values()]

    if hyperbolic:
        # Only draw the nodes
        nx.draw_networkx_nodes(
            network,
            ax=ax,
            pos=pos,
            node_color=node_color,
            node_size=node_size,
            linewidths=2.0  # node borders
            )

        p=node_pos_dict2array(pos)

        if bkst: # Add jitter to Bookstein coordinates for plottability
            p[:2,:] += np.random.normal(0, 1e-6, size=(2,2))

        # draw curved edges
        plot_hyperbolic_edges(p=p, A=A, ax=ax, R=disk_radius, linewidth=edge_width, threshold=threshold, continuous=continuous, bkst=bkst)

    else: # Draw straight edges
        nx.draw_networkx(
            network,
            ax=ax,
            pos=pos,
            node_color=node_color,
            node_size=node_size,
            linewidths=2.0,  # node borders
            edge_color='#333333',
            width=edge_width,  # edge widths
            with_labels=False)

    if title is not None:
        ax.set_title(title, color='k', fontsize='24')
    margin = 1+margin
    ax.set(xlim=(-margin*disk_radius,margin*disk_radius),ylim=(-margin*disk_radius,margin*disk_radius))
    ax.axis('off')
    if not clean_ax:
        plt.tight_layout()
        plt.show()
    return ax

def node_pos_dict2array(pos_dict:dict) -> np.ndarray:
    """
    Puts the dictionary latent positions {node: position} into an (n,D) array
    """
    n = len(pos_dict)
    D = len(pos_dict[0])
    pos_array = np.zeros((n, D))
    for i in range(n):
        pos_array[i, :] = pos_dict[i]
    return pos_array

def lorentz_to_poincare(networks:ArrayLike) -> jnp.ndarray:
    """
    Convert Lorentz coordinates to Poincaré coordinates, eq 11 from Nickel & Kiela (2018).
    PARAMS:
    network (S,N,D) or (N,D): numpy array with Lorentzian coordinates [samples x nodes x dimensions] or [nodes x dimensions]
    """
    one_nw = len(networks.shape) == 2
    if one_nw: # We pass 1 network
        networks = np.array([networks])

    S, N, D_L = networks.shape
    calc_z_P = lambda c, nw: (None, nw[:,1:]/np.reshape(np.repeat(nw[:,0]+1, D_L-1), newshape=(N, D_L-1)))
    _, z_P = jax.lax.scan(calc_z_P, None, networks)

    return z_P[0,:,:] if one_nw else z_P

def triu2mat(v:ArrayLike) -> jnp.ndarray:
    """
    Fills a matrix from upper triangle vector
    PARAMS:
    v (ArrayLike) : upper triangle vector
    """
    m = len(v)
    n = int((1 + np.sqrt(1 + 8 * m)) / 2)
    mat = np.zeros((n, n))
    triu_indices = np.triu_indices(n, k=1)
    mat[triu_indices] = v
    return mat + mat.T

def hyp_pnt(X:ArrayLike) -> jnp.ndarray:
    """
    Create a 3D point [z,x,y] in hyperbolic space by projecting onto hyperbolic plane from 2D X=[x,y]
    PARAMS
    X : array containing 2D points to be projected up onto the hyperbolic plane
    """
    N, D = X.shape
    z = jnp.sqrt(jnp.sum(X**2, axis=1)+1)
    x_hyp = jnp.zeros((N,D+1))
    x_hyp = x_hyp.at[:,0].set(z)
    x_hyp = x_hyp.at[:,1:].set(X)
    return x_hyp

def invlogit(x:ArrayLike) -> ArrayLike:
    """
    Definition of the inverse-logit function (a.k.a. the logistic function)
    PARAMS:
    x : input variables
    """
    return 1 / (1 + jnp.exp(-x))

def lorentzian(v:ArrayLike, u:ArrayLike, keepdims:bool=False) -> jnp.ndarray:
    """
    Returns the Lorentzian prodcut of v and u, defined as -v_0*u_0 = SUM_{i=1}^N v_i*u_i
    PARAMS:
    v (N,D) : vector
    u (N,D) : vector
    """
    signs = jnp.ones_like(v)
    signs = signs.at[:,0].set(-1)
    return jnp.sum(v*u*signs, axis=1, keepdims=keepdims)

def lorentz_distance(z:ArrayLike) -> jnp.ndarray:
    """
    Returns the hyperbolic distance between all N points in z as a N x N matrix
    PARAMS:
    z : points in hyperbolic space
    """
    def arccosh(x:ArrayLike) -> jnp.ndarray:
        """
        Definition of the arccosh function
        PARAMS:
        x : input
        """
        x_clip = jnp.clip(x, 1, x)
        return jnp.log(x_clip + jnp.sqrt(x_clip**2 - 1))
    signs = jnp.ones_like(z)
    signs = signs.at[:,0].set(-1)
    lor = jnp.dot(signs*z, z.T)
    ## Due to numerical instability, we can get nan's on the diagonal!
    dist = arccosh(-lor)
    dist = dist.at[jnp.diag_indices_from(dist)].set(0)
    return dist

def parallel_transport(v:ArrayLike, nu:ArrayLike, mu:ArrayLike) -> ArrayLike:
    """
    Parallel transports the points v sampled around nu to the tangent space of mu
    PARAMS:
    v  (N,D) : points on tangent space of nu [points on distribution around nu]
    nu (N,D) : point in hyperbolic space [center to move from] (mu_0 in Fig 2 of Nagano)
    mu (N,D) : point in hyperbolic space [center to move to]
    """
    alpha = -lorentzian(nu, mu, keepdims=True)
    u = v + lorentzian(mu - alpha*nu, v, keepdims=True)/(alpha+1) * (nu + mu)
    return u

def exponential_map(mu:ArrayLike, v:ArrayLike, eps:float=1e-6) -> ArrayLike:
    """
    Maps the points v on the tangent space of mu onto the hyperolic plane
    PARAMS:
    mu (N,D) : Transported middle points
    v (N,D) : Points to be mapped onto hyperbolic space
    eps : minimum value
    """
    # Euclidean norm from mu_0 to v is the same as from mu to u is the same as the Hyp-norm from mu to exp_mu(u), hence we can use the Euclidean norm of v.
    lor = lorentzian(v,v,keepdims=True)
    v_norm = jnp.sqrt(jnp.clip(lor, eps, lor))  ## If eps is too small, it gets rounded right back to zero and then we divide by zero
    return jnp.cosh(v_norm) * mu + jnp.sinh(v_norm) * v / v_norm

# Global parameters
eps = 1e-6
mu = jnp.array([0.,0.])
sigma = 1.
mu_sigma = 0.
sigma_sigma = 1.

"""## Functions to calculate parameters"""

def get_det_params(_z:ArrayLike, dirty:bool=True, mu:ArrayLike=mu, eps:float=eps) -> ArrayLike:
    """
    Calculates all deterministicly dependent parameters, up to and including the bound of sigma_beta, and returns those in a dictionary.
    PARAMS:
    _z : 2D Gaussian, to be projected onto the hyperbolic plane
    dirty : whether the prior is a dirty prior (projected onto hyperbolic plane) or a proper prior (Nagano et al. wrapped hyperbolic normal)
    mu : mean of the wrapped hyperbolic normal prior
    eps : offset for calculating d_norm, to insure max(d_norm) < 1
    """
    N, D = _z.shape
    triu_indices = jnp.triu_indices(N, k=1)

    if dirty:
        z = hyp_pnt(_z)
    else:
        mu_0 = jnp.zeros((N, D+1))
        mu_0 = mu_0.at[:,0].set(1)

        if hasattr(mu, "__len__"):
            assert len(mu) == D, 'Dimension of mu must correspond to the dimension of each point in _z'
            mu_tilde = jnp.reshape(jnp.tile(mu, N), (N,D))
        else:
            mu_tilde = mu*jnp.ones_like(_z)
        mu = hyp_pnt(mu_tilde) # UGLY?! How else to get a proper mean in H?
        v = jnp.concatenate([jnp.zeros((N,1)), _z], axis=1)
        u = parallel_transport(v, mu_0, mu)
        z = exponential_map(mu, u)

    d = lorentz_distance(z)[triu_indices]
    d_norm = d/(jnp.max(d)+eps)
    mu_beta = 1-d_norm
    bound = jnp.sqrt(mu_beta*(1-mu_beta))

    params = {'_z':_z,
              'z':z,
              'd':d,
              'd_norm':d_norm,
              'mu_beta':mu_beta,
              'bound':bound}
    return params

def get_ab(mu_beta:ArrayLike, sigma_beta:float) -> tuple:
    """
    Calculates the a and b parameters for the beta distribution
    PARAMS:
    mu_beta : mean of the beta distribution
    sigma_beta : standard deviations of the beta distribution
    """
    kappa = mu_beta*(1-mu_beta)/sigma_beta**2 - 1
    a = mu_beta*kappa
    b = (1-mu_beta)*kappa
    return a, b

def get_all_params(_z:ArrayLike, sigma_beta:float, A:ArrayLike=None, eps:float=eps) -> dict:
    """
    Returns a dictionary with all parameters, given the sampled parameters. If A is given, it is added.
    PARAMS:
    _z : prior positions
    sigma_beta : standard deviation of the beta distribution
    A : upper triangle of the observed adjacency matrix
    eps : offset for calculating d_norm, to insure max(d_norm) < 1
    """
    params = get_det_params(_z, eps)
    mu_beta = params['mu_beta']
    a,b = get_ab(mu_beta, sigma_beta)
    params.update({'a':a,
                   'b':b})
    if A is not None:
        params.update({'A':A})
    return params

"""## Sampling functions"""

def sample_prior(key:PRNGKeyArray, shape:tuple, mu:ArrayLike=None, sigma:float=sigma, mu_sigma:float=mu_sigma, sigma_sigma:float=sigma_sigma) -> dict:
    """
    Samples z positions from the Wrapped Hyperbolic normal distribution (Nagano et al. 2019)
    Returns the prior parameters in a dictionary.
    PARAMS:
    key : random key for JAX functions
    shape : shape of the prior positions
    sigma : standard deviation of the 2D Gaussian to sample mu
    mu_sigma : mean of the 1D Gaussian to sample sigma_T
    sigma_sigma : standard deviation of the 1D Gaussian to sample sigma_T
    """
    key = jax.random.split(key, num=1)[0]
    mu = 0 if mu is None else mu
    _z = sigma*jax.random.normal(key, shape=shape)+mu # Is always centered at mu=0

    key = jax.random.split(key, num=1)[0]
    sigma_beta_T = sigma_sigma*jax.random.normal(key, (1,)) + mu_sigma

    prior = {'_z':_z,
            'sigma_beta_T':sigma_beta_T}
    return key, prior

def sample_observation(key:PRNGKeyArray, prior:dict, n_samples:int=1, dirty=True, eps:float=eps) -> dict:
    """
    Generates an observation based on the prior
    PARAMS:
    key : random key for JAX functions
    prior : dictionary containing sampled variables from the prior ('_z', 'sigma_beta_T')
    n_samples : number of observations to sample
    eps : offset for the normalization of d, and clipping of A
    """
    # Get prior position and sigma
    _z, sigma_beta_T = prior['_z'], prior['sigma_beta_T']
    N = _z.shape[0]
    M = N*(N-1)//2

    # Calculate mu and bound
    params = get_det_params(_z, dirty=dirty, eps=eps)
    mu_beta, bound = params['mu_beta'], params['bound']

    # Transform sigma_beta back
    sigma_beta = invlogit(sigma_beta_T)*bound

    # Calculate a, b parameters
    a, b = get_ab(mu_beta, sigma_beta)

    # Sample A, and clip between eps and 1-eps
    key = jax.random.split(key, num=1)[0]
    A = jnp.clip(jax.random.beta(key, a, b, shape=(n_samples, M,)), eps, 1-eps)

    return key, A

"""## Probability density definitions"""

def log_prior(_z:ArrayLike, sigma_beta_T:float, mu:float=mu, sigma:float=sigma, mu_sigma:float=mu_sigma, sigma_sigma:float=sigma_sigma) -> float:
    """
    Returns the (dirty) log-prior
    PARAMS:
    _z : x,y positions of the positions
    sigma_beta_T : transformed standard deviations of the beta distribution
    mu : mean of the 2D Gaussian that is projected to the hyperbolic plane
    sigma : standard deviation of the 2D Gaussian that is projected to the hyperbolic plane
    mu_sigma :
    sigma_sigma :
    """
    logprob__z = jstats.norm.logpdf(_z, loc=mu, scale=sigma).sum()
    logprob_sigma_T = jstats.norm.logpdf(sigma_beta_T, loc=mu_sigma, scale=sigma_sigma).sum()
    return logprob__z + logprob_sigma_T

def log_likelihood(_z:ArrayLike, sigma_beta_T:float, obs:ArrayLike, dirty=True, eps:float=eps) -> float:
    """
    Returns the log-likelihood
    PARAMS:
    _z : x,y positions of the positions
    sigma_beta_T : transformed standard deviations of the beta distribution
    obs : observed correlations (samples x edges)
    dirty : whether the prior is a dirty prior (projected onto hyperbolic plane) or a proper prior (Nagano et al. wrapped hyperbolic normal)
    eps : offset for calculating d_norm, to insure max(d_norm) < 1
    """
    params = get_det_params(_z, dirty=dirty, eps=eps)
    mu_beta, bound = params['mu_beta'], params['bound']

    sigma_beta = invlogit(sigma_beta_T)*bound # Transform sigma_beta back to [0, bound] to get a,b
    a,b = get_ab(mu_beta, sigma_beta)
    logprob_A = jstats.beta.logpdf(obs, a, b).sum() # S x M obs --> scalar logprob_A
    return logprob_A

def log_density(_z:ArrayLike, sigma_beta_T:float, obs:ArrayLike, dirty=True, mu:ArrayLike=mu, sigma:float=sigma, eps:float=eps) -> float:
    """
    Returns the log-probability density of the observed edge weights under the Continuous Hyperbolic LSM.
    PARAMS:
    _z : positions on Euclidean plane (pre hyperbolic projection)
    sigma_beta_T : transformed standard deviations of the beta distributions
    obs : observed correlations (samples x edges)
    mu : mean of the 2D Gaussian that is projected to the hyperbolic plane
    sigma : standard deviation of the 2D Gaussian that is projected to the hyperbolic plane
    eps : offset for calculating d_norm, to insure max(d_norm) < 1
    """
    prior_prob = log_prior(_z, sigma_beta_T, mu, sigma)
    likelihood_prob = log_likelihood(_z, sigma_beta_T, obs, dirty, eps) # (samples,)
    return prior_prob + likelihood_prob

"""## Random-walk metropolis"""

# Define the sampling loop
def rmh_sampling_loop(key:PRNGKeyArray, kernel:Callable, initial_state:dict, num_samples:int):
    """
    Uses JAX to sample the posterior that follows the log-density function given to the kernel.
    PARAMS:
    key : Random key for JAX functions
    kernel : kernel to get the next MCMC position
    initial_state : initial MCMC position
    num_samples : number of MCMC samples to take
    """
    @jax.jit
    def one_step(state, key):
        state, state_info = kernel(key, state)
        full_state = {'state':state,'state_info':state_info}
        return state, full_state # We return the last state and the list of states with their info

    keys = jax.random.split(key, num_samples+1)
    last_state, states= jax.lax.scan(one_step, initial_state, keys[:-1]) # <- first returned argument is the last state, second returned argument is the collected list

    return keys[-1], last_state, states

N = 5
D = 2
M = N*(N-1)//2
triu_indices = np.triu_indices(N,k=1)

n_observations = 10

# Initialize PRNGKey
key = jax.random.PRNGKey(1234)

# Create observation
key, gt_prior = sample_prior(key, (N,D))
key, obs = sample_observation(key, gt_prior, n_samples=n_observations, dirty=False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# key, prior = sample_prior(key,(N,D))
# initvals = {'_z':prior['_z'],
#             'sigma_beta_T':prior['sigma_beta_T']}
# 
# lambda_log_density = lambda x: log_density(**x, dirty=False, obs=obs)
# stepsize = 1e-2*jnp.eye(N*D+1)
# 
# rmh = bjx.rmh(lambda_log_density, stepsize)
# initial_state = rmh.init(initvals)
# 
# n_steps = 250_000
# 
# key, last_state, trace = rmh_sampling_loop(key, rmh.step, initial_state, n_steps)

size = 5
theta = 0.0 # Just plot everything

n_samples = 5
sample_idx = np.zeros(n_samples, dtype=int) # Always add initial position
sample_idx[-1] = n_steps-1 # And add last position
sample_idx[1:n_samples-1] = np.sort(np.random.randint(1, n_steps, size=n_samples-2))

obs_pidx = np.random.randint(n_observations)

# Observed & prior values
gt_params = get_det_params(gt_prior['_z'])
gt_pos_poincare = lorentz_to_poincare(gt_params['z'])
gt_d = triu2mat(gt_params['d'])
gt_edge_weights = triu2mat(obs[obs_pidx])
obs_network = nx.from_numpy_array(gt_edge_weights)

nrows, ncols = 3,1+n_samples
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*size, nrows*size))

# Plot prior network
node_color = D*['r']+(N-D)*['k']
prior_pos_dict = {j: gt_pos_poincare[j, :] for j in range(N)}
plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,0],
                    title='Ground truth network',  threshold=theta, node_color=node_color)
poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
axes[0,0].add_patch(poincare_disk)

# Plot prior distances
axes[1,0].imshow(gt_d)
axes[1,0].set_title('Ground truth distances')

# Plot observed correlations
axes[2,0].imshow(gt_edge_weights)
axes[2,0].set_title('Observed correlations (sample {})'.format(obs_pidx))
axes[2,0].set_xlabel('Node')
axes[2,0].set_ylabel('Node')

for i, s in enumerate(sample_idx):
    # Get values
    state_params = get_det_params(trace['state'].position['_z'][s,:,:])
    sample_pos = lorentz_to_poincare(state_params['z'])
    sampled_d = triu2mat(state_params['d'])

    # Plot sampled network
    prior_pos_dict = {j: sample_pos[j, :] for j in range(N)}
    plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,i+1],
                        title='Sample {} positions'.format(s),  threshold=theta, node_color=node_color)
    poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
    axes[0,i+1].add_patch(poincare_disk)

    # Plot sampled distances
    axes[1,i+1].imshow(sampled_d)
    axes[1,i+1].set_title('Sample {} distances'.format(s))

    # Turn off last axis
    axes[2,i+1].scatter(gt_params['d'], state_params['d'], c='k')
    axes[2,i+1].set_xlabel('Ground truth D')
    axes[2,i+1].set_ylabel('Sampled D')

plt.tight_layout()
savetitle = 'HypCon_RMH_samples.png'
plt.savefig(savetitle)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gt_d_triu = gt_d[triu_indices]
# get_corr = lambda carry, position: (None, jnp.corrcoef(gt_d_triu, lorentz_distance(hyp_pnt(position))[triu_indices])[0,1])
# 
# _, corrs = jax.lax.scan(get_corr, None, trace['state'].position['_z'])

burn_in = 25_000
plt.figure(figsize=(8,5))
plt.plot(range(n_steps), corrs)
ymin, ymax = plt.ylim()
plt.vlines(burn_in, ymin, ymax, colors=['k'])
plt.xlabel('step')
plt.ylabel('Correlation')
title = 'Correlations between the distance matrices of the trace and ground truth (RMH)'
plt.title(title)
savetitle = 'RMH_Correlations.png'
plt.savefig(savetitle)
plt.show()

n_bins = 100
plt.figure(figsize=(8,5))
rmh_hist, rmh_bins = jnp.histogram(corrs[burn_in:], bins=n_bins) # Gebruik jax wants snel
plt.stairs(rmh_hist, rmh_bins, fill=True)
# plt.hist(corrs[burn_in:], bins=n_bins)
plt.xlabel('Correlation')
plt.ylabel('Count')
title = 'Distance correlation (RMH)'
plt.title(title)
savetitle = 'RMH_Correlations_hist.png'
plt.savefig(savetitle)
plt.show()

"""### Bookstein coordinates"""

def get_bookstein_target(n_dims:int = 2, offset:float=0.3) -> jnp.ndarray:
    """
    Returns the bookstein coordinate target for the first two positions, in n_dims dimensions
    PARAMS:
    n_dims : number of dimensions of the latent space
    offset : offset on the x-axis that the Bookstein targets are put. Node 1 is put on (-offset, 0), node 2 on (offset, 0).
    """
    assert offset > 0., 'Offset must be bigger than 0 but is {}'.format(offset)
    bookstein_target = jnp.zeros(shape=(2, n_dims)) # 2 for the first 2 positions
    bookstein_target = bookstein_target.at[0,0].set(-offset)
    bookstein_target = bookstein_target.at[1,0].set(offset)
    return bookstein_target

def bookstein_log_prior(_z:ArrayLike, sigma_beta_T:float, dirty=True, mu:float=mu, sigma:float=sigma, mu_sigma:float=mu_sigma, sigma_sigma:float=sigma_sigma) -> float:
    """
    Returns the log-prior for a network with bookstein coordinates
    PARAMS:
    _z : x,y positions of the positions
    sigma_beta_T : transformed standard deviations of the beta distribution
    mu : mean of the 2D Gaussian that is projected to the hyperbolic plane
    sigma : standard deviation of the 2D Gaussian that is projected to the hyperbolic plane
    mu_sigma : mean of the Gaussian of transformed sigma
    sigma_sigma : standard deviation of the Gaussian of transformed sigma
    """
    if not dirty:
        mu = 0.
    rest_logprior = log_prior(_z[1:,:], sigma_beta_T, mu, sigma, mu_sigma, sigma_sigma) # Use all sigma_T's, not all _z values
    pivot_point_logprior = jnp.log(2)+jstats.norm.logpdf(_z[0,:], loc=mu, scale=sigma).sum() + np.NINF*(_z[0,1] < 0.) # log[2*N(_z[2]|mu,sigma)] if _z[2] >= 0 else -INF.

    return rest_logprior+pivot_point_logprior

def bookstein_log_likelihood(_z:ArrayLike, sigma_beta_T:float, obs:ArrayLike, dirty:bool=True, eps:float=eps) -> float:
    """
    Returns the log-likelihood
    PARAMS:
    _z : x,y positions of the positions
    sigma_beta_T : transformed standard deviations of the beta distribution
    obs : observed correlations (samples x edges)
    """
    n_dims = _z.shape[1]
    bookstein_target = get_bookstein_target(n_dims)

    # Concatenate bookstein targets to _z
    _zc = jnp.concatenate([bookstein_target, _z])

    # Get parameters from _z with concatenated bkst
    params = get_det_params(_zc, dirty=dirty, eps=eps)
    mu_beta, bound = params['mu_beta'], params['bound']

    sigma_beta = invlogit(sigma_beta_T)*bound # Transform sigma_beta back to [0, bound] to get a,b
    a, b = get_ab(mu_beta, sigma_beta)
    logprob_A = jstats.beta.logpdf(obs, a, b).sum() # S x M obs --> scalar logprob_A
    return logprob_A

def bookstein_position(_z:ArrayLike) -> ArrayLike:
    """
    Turns a set of _z positions into bookstein coordinates, where the first position is constrained. The two Bookstein targets are implicit.
    PARAMS:
    position (ArrayLike) : _z position in the latent space
    """
    n_nodes, n_dims = _z.shape
    # Whether to flip that particle around the y-axis
    _do_flip = _z[0,1] < 0 # Is first (third when counting implicit bkst) _z below x-axis?
    do_flip = jnp.reshape(jnp.repeat(jnp.repeat(_do_flip, n_dims),n_nodes), (n_nodes, n_dims)) # Don't ask.

    # The flip operator over the x-axis
    _x_flip = jnp.array([1,-1])
    x_flip = jnp.tile(_x_flip, (n_nodes,1))

    # Then flip over x-axis if flip, or keep the same if no flip
    _z = _z*x_flip*do_flip + _z*(1-do_flip)
    return _z

def bookstein_init(bkst_init:RMHState, prior:dict,log_density:Callable) -> PyTree:
    """
    Initializes the prior to start in bookstein position.
    PARAMS:
    bkst_init : initialization function used of the RMH kernel
    prior : prior dictionary containing positions _z
    log_density : log density function to determine the probability of going to the proposed position
    """
    N,D = prior['_z'].shape
    assert N > D, "Must have more than D positions to use Bookstein coordinates, but N={} and D={}".format(N,D)
    prior['_z'] = prior['_z'][2:,:] # Cut off first two positions, the bookstein targets are implicit
    initial_state = bkst_init(position=prior, logprob_fn=log_density)
    initial_state.position['_z'] = bookstein_position(initial_state.position['_z'])
    return initial_state

def rmh_bkst_sampling_loop(key, step_fn, log_density, initial_state, parameters, num_samples):
    """
    Uses JAX to sample the posterior that follows the log-density function given to the kernel.
    PARAMS:
    key : Random key for JAX functions
    step_fn : step function to get the next MCMC position
    log_density : log density function to determine the probability of going to the proposed position
    initial_state : initial MCMC position
    parameters : extra parameters needed for the step function (e.g. stepsize)
    num_samples : number of MCMC samples to take
    """
    @jax.jit
    def one_step(state, key):
        state, state_info = step_fn(
            rng_key=key,
            state=state,
            logprob_fn=log_density,
            **parameters
        )
        full_state = {'state':state,'state_info':state_info}
        return state, full_state

    keys = jax.random.split(key, num_samples+1)
    last_state, states= jax.lax.scan(one_step, initial_state, keys[:-1])

    return keys[-1], last_state, states

N = 5
D = 2
M = N*(N-1)//2
n_observations = 10
triu_indices = np.triu_indices(N,k=1)

# Initialize PRNGKey
key = jax.random.PRNGKey(1234)

# Create observation
key, gt_prior = sample_prior(key, (N,D))
key, obs = sample_observation(key, gt_prior, n_observations, dirty=False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# _log_density = lambda state: bookstein_log_prior(**state) + bookstein_log_likelihood(**state, dirty=False, obs=obs) # Both prior and likelihood take _z, sigma in that order
# key, prior = sample_prior(key, (N,D))
# 
# bkst_init = bjx.mcmc.rmh.init
# bkst_step = bjx.mcmc.rmh.kernel()
# 
# initial_state = bookstein_init(bkst_init, prior, _log_density)
# 
# stepsize = 1e-2*jnp.eye((N-D)*D+1)
# parameters = {'sigma':stepsize}
# 
# n_steps = 1_000_001
# 
# key, last_state, trace = rmh_bkst_sampling_loop(key, bkst_step, _log_density, initial_state, parameters, n_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Add bkst position to each position in trace
# bkst_target = get_bookstein_target(D)
# add_bkst = lambda c, s : (None, jnp.concatenate([bkst_target, trace['state'].position['_z'][s,:,:]]))
# _, bkst_z_pos = jax.lax.scan(add_bkst, None, jnp.arange(n_steps))
# trace['state'].position['_z'] = bkst_z_pos

size = 5
theta = 0.0 # Just plot everything

n_trace_samples = 5
sample_idx = np.zeros(n_trace_samples, dtype=int) # Always add initial position
sample_idx[-1] = n_steps-1 # And add last position
sample_idx[1:n_trace_samples-1] = np.sort(np.random.randint(1, n_steps, size=n_trace_samples-2))

obs_pidx = np.random.randint(n_observations)

# Observed & prior values
gt_params = get_det_params(gt_prior['_z'])
gt_pos_poincare = lorentz_to_poincare(gt_params['z'])
gt_d = triu2mat(gt_params['d'])
gt_edge_weights = triu2mat(obs[obs_pidx])
obs_network = nx.from_numpy_array(gt_edge_weights)

nrows, ncols = 3,1+n_trace_samples
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*size, nrows*size))

# Plot prior network
node_color = D*['r']+(N-D)*['k']
prior_pos_dict = {j: gt_pos_poincare[j, :] for j in range(N)}
plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,0],
                    title='Ground truth network',  threshold=theta, node_color=node_color)
poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
axes[0,0].add_patch(poincare_disk)

# Plot prior distances
axes[1,0].imshow(gt_d)
axes[1,0].set_title('Ground truth distances')

# Plot observed correlations
axes[2,0].imshow(gt_edge_weights)
axes[2,0].set_title('Observed correlations (sample {})'.format(obs_pidx))
axes[2,0].set_xlabel('Node')
axes[2,0].set_ylabel('Node')

bkst_target = get_bookstein_target(D)

for i, s in enumerate(sample_idx):
    # Get values
    state_params = get_det_params(trace['state'].position['_z'][s,:,:])
    sample_pos = lorentz_to_poincare(state_params['z'])
    sampled_d = triu2mat(state_params['d_norm'])

    # Plot sampled network
    prior_pos_dict = {j: sample_pos[j, :] for j in range(N)}
    plot_network(network=obs_network, pos=prior_pos_dict, ax=axes[0,i+1], title='Sample {} positions'.format(s),
                 hyperbolic=True, continuous=True, bkst=True, threshold=theta, node_color=node_color)
    poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
    axes[0,i+1].add_patch(poincare_disk)

    # Plot sampled distances
    axes[1,i+1].imshow(sampled_d)
    axes[1,i+1].set_title('Sample {} distances'.format(s))

    # Turn off last axis
    axes[2,i+1].scatter(gt_params['d_norm'], state_params['d_norm'], c='k')
    axes[2,i+1].set_xlabel('Ground truth D (normalized)')
    axes[2,i+1].set_ylabel('Sampled D (normalized)')

plt.tight_layout()
title = 'HypCon_RMH_BooksteinLearning.png'
plt.savefig(title)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gt_d_triu = gt_d[triu_indices]
# get_corr = lambda carry, position: (None, jnp.corrcoef(gt_d_triu, lorentz_distance(hyp_pnt(position))[triu_indices])[0,1])
# 
# _, corrs = jax.lax.scan(get_corr, None, trace['state'].position['_z'])

burn_in = 25_000
plt.figure(figsize=(8,5))
plt.plot(range(n_steps), corrs)
ymin, ymax = plt.ylim()
plt.vlines(burn_in, ymin, ymax, colors=['k'])
plt.xlabel('step')
plt.ylabel('Correlation')
title = 'Correlations between the distance matrices of the trace and ground truth (RMH)'
plt.title(title)
savetitle = 'Bookstein_Correlations.png'
plt.savefig(savetitle)
plt.show()

n_bins = 100
plt.figure(figsize=(8,5))
rmh_bkst_hist, rmh_bkst_bins = jnp.histogram(corrs[burn_in:], bins=n_bins) # Gebruik jax wants snel
plt.stairs(rmh_bkst_hist, rmh_bkst_bins, fill=True)
plt.xlabel('Correlation')
plt.ylabel('Count')
title = 'Distance correlation histogram for all {:,} steps'.format(n_steps)
plt.title(title)
savetitle = 'Bookstein_Correlations_hist.png'
plt.savefig(savetitle)
plt.show()

"""## SMC"""

def smc_inference_loop(key:PRNGKeyArray, smc_kernel:Callable, initial_state:dict):
    """
    Run the temepered SMC algorithm.

    Run the adaptive algorithm until the tempering parameter lambda reaches the value lambda=1.
    PARAMS:
    key : random key for JAX functions
    smc_kernel : kernel for the SMC particles
    initial_state : initial position and temperature for the SMC particles
    """
    def cond(carry):
        _, state, _ = carry
        return state.lmbda < 1

    @jax.jit
    def step(carry):
        i, state, key = carry
        key, subkey = jax.random.split(key)
        state, _ = smc_kernel(subkey, state)
        return i+1, state, key

    n_iter, final_state, key = jax.lax.while_loop(
        cond, step, (0, initial_state, key)
    )

    return key, n_iter, final_state

def initialize_particles(key:PRNGKeyArray, num_particles:int, shape:tuple, dirty=True, mu:float=mu, sigma:float=sigma, mu_sigma:float=mu_sigma, sigma_sigma:float=sigma_sigma):
    """
    Initializes the SMC particles. Equivalent to sample_prior, but with an extra dimension for the SMC particles.
    PARAMS:
    key : PRNGKeyArray to be used as random key for JAX functions
    num_particles : number of SMC particles
    shape : number of nodes by number of dimensions
    mu : mean of the positions' 2D Gaussian
    sigma : std of the positions' 2D Gaussian
    mu_sigma : mean of the transformed std distribution
    sigma_sigma : std of the transformed std distribution
    """
    N, D = shape
    if not dirty:
        mu = 0.
    _z_key, sigma_beta_T_key, key = jax.random.split(key, 3)
    initial_position = {'_z': sigma*jax.random.normal(_z_key, shape=(num_particles, N, D))+mu,
                        'sigma_beta_T': sigma_sigma*jax.random.normal(sigma_beta_T_key, shape=(num_particles, 1))+mu_sigma}
    return key, initial_position

N = 20
D = 2
M = N*(N-1)//2
triu_indices = np.triu_indices(N,k=1)
n_observations = 10

# Initialize PRNGKey
key = jax.random.PRNGKey(1234)

# Create observation
key, gt_prior = sample_prior(key, (N,D))
key, obs = sample_observation(key, gt_prior, n_samples=n_observations, dirty=False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Density functions
# _log_prior = lambda state: log_prior(**state)
# _log_likelihood = lambda state: log_likelihood(**state, dirty=False, obs=obs)
# 
# # Kernel parameters
# rwm_parameters = {'sigma': 1e-2*jnp.eye(N*D+1)}
# 
# # RWM within SMC kernel
# smc = bjx.adaptive_tempered_smc(
#     logprior_fn = _log_prior,
#     loglikelihood_fn = _log_likelihood,
#     mcmc_algorithm = bjx.rmh,
#     mcmc_parameters = rwm_parameters,
#     resampling_fn = resampling.systematic,
#     target_ess = 0.5,
#     mcmc_iter = 1_000,
# )
# 
# num_particles = 1_000
# 
# init_key = jax.random.PRNGKey(0)
# key, init_position = initialize_particles(init_key, num_particles, (N,D))
# initial_smc_state = smc.init(init_position)
# key, n_iter, states_rwm_smc = smc_inference_loop(key, smc.step, initial_smc_state)

size = 5
theta = 0.0 # Just plot everything

n_trace_samples = 5
sample_idx =  np.sort(np.random.randint(1, num_particles, size=n_trace_samples))

obs_pidx = np.random.randint(n_observations)

# Observed & prior values
gt_params = get_det_params(gt_prior['_z'])
gt_pos_poincare = lorentz_to_poincare(gt_params['z'])
gt_d = triu2mat(gt_params['d'])
gt_edge_weights = triu2mat(obs[obs_pidx])
obs_network = nx.from_numpy_array(gt_edge_weights)

nrows, ncols = 3,1+n_trace_samples
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*size, nrows*size))

# Plot prior network
prior_pos_dict = {j: gt_pos_poincare[j, :] for j in range(N)}
plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,0],
                    title='Ground truth network',  threshold=theta)
poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
axes[0,0].add_patch(poincare_disk)

# Plot prior distances
axes[1,0].imshow(gt_d)
axes[1,0].set_title('Ground truth distances')

# Plot observed correlations
axes[2,0].imshow(gt_edge_weights)
axes[2,0].set_title('Observed correlations (sample {})'.format(obs_pidx))
axes[2,0].set_xlabel('Node')
axes[2,0].set_ylabel('Node')

for i, s in enumerate(sample_idx):
    # Get values
    state_params = get_det_params(states_rwm_smc.particles['_z'][s,:,:])
    sample_pos = lorentz_to_poincare(state_params['z'])
    sampled_d = triu2mat(state_params['d'])

    # Plot sampled network
    prior_pos_dict = {j: sample_pos[j, :] for j in range(N)}
    plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,i+1],
                        title='Sample {} positions'.format(s),  threshold=theta)
    poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
    axes[0,i+1].add_patch(poincare_disk)

    # Plot sampled distances
    axes[1,i+1].imshow(sampled_d)
    axes[1,i+1].set_title('Sample {} distances'.format(s))

    # Turn off last axis
    axes[2,i+1].scatter(gt_params['d'], state_params['d'], c='k')
    axes[2,i+1].set_xlabel('Ground truth D')
    axes[2,i+1].set_ylabel('Sampled D')

plt.tight_layout()
savetitle = 'SMC_embedding.png'
plt.savefig(savetitle)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gt_d_triu = gt_d[triu_indices]
# get_corr = lambda carry, position: (None, jnp.corrcoef(gt_d_triu, lorentz_distance(hyp_pnt(position))[triu_indices])[0,1])
# 
# _, corrs = jax.lax.scan(get_corr, None, states_rwm_smc.particles['_z'])

plt.figure(figsize=(8,5))
plt.hist(corrs, bins=n_bins)
plt.xlabel('Correlation')
plt.ylabel('Count')
title = 'Distance correlation histogram of all {:,} particles'.format(num_particles)
plt.title(title)
savetitle = 'SMC_Correlations_hist.png'
plt.savefig(savetitle)
plt.show()

"""### Bookstein within SMC"""

def smc_bookstein_position(position:ArrayLike) -> ArrayLike:
    """
    Turns a set of _z positions into bookstein coordinates, where the first two positions are set, and the third position is constrained.
    Keeps particle dimension in mind.
    PARAMS:
    position : _z position in the latent space
    """
    n_particles, n_nodes, n_dims = position.shape

    ## Set 3rd position to be on one possible side of the x-axis

    # Whether to flip that particle around the y-axis
    _do_flip = position[:,0,1] < 0 # Is first (third with implicit bkst coords) position below x-axis?
    do_flip = jnp.reshape(jnp.repeat(jnp.repeat(_do_flip, n_dims),n_nodes), (n_particles, n_nodes, n_dims))

    # The flip operator over the x-axis
    _x_flip = jnp.array([1,-1])
    x_flip = jnp.tile(_x_flip, (n_particles,n_nodes,1))

    # Then flip over x-axis if flip, or keep the same if no flip
    position = position*x_flip*do_flip + position*(1-do_flip)
    return position

def smc_bkst_inference_loop(key:PRNGKeyArray, smc_kernel:Callable, initial_state:ArrayLike) -> tuple:
    """
    Run the temepered SMC algorithm with Bookstein anchoring.

    Run the adaptive algorithm until the tempering parameter lambda reaches the value lambda=1.
    PARAMS:
    key: random key for JAX functions
    smc_kernel: kernel for the SMC particles
    initial_state: beginning position of the algorithm
    """
    def cond(carry):
        _, state, _ = carry
        return state.lmbda < 1

    @jax.jit
    def step(carry):
        i, state, key = carry
        key, subkey = jax.random.split(key)
        state, _ = smc_kernel(subkey, state)
        return i+1, state, key

    n_iter, final_state, key = jax.lax.while_loop(
        cond, step, (0, initial_state, key)
    )

    return key, n_iter, final_state

def initialize_bkst_particles(key:PRNGKeyArray, num_particles:int, shape:tuple, dirty=True, mu:float=mu, sigma:float=sigma, mu_sigma:float=mu_sigma, sigma_sigma:float=sigma_sigma) -> tuple:
    """
    Initializes the SMC particles, but with Bookstein coordinates.
    PARAMS:
    key : random key for JAX functions
    num_particles : number of SMC particles
    shape : number of nodes by number of dimensions
    mu : mean of the positions' 2D Gaussian
    sigma : std of the positions' 2D Gaussian
    mu_sigma : mean of the transformed std distribution
    sigma_sigma : std of the transformed std distribution
    """
    N, D = shape
    if not dirty:
        mu = 0.
    _z_key, sigma_beta_T_key, key = jax.random.split(key, 3)
    initial_position = {'_z': smc_bookstein_position(sigma*jax.random.normal(_z_key, shape=(num_particles, N-D, D))+mu), # N-D to skip first D bkst nodes (they are implicit)
                        'sigma_beta_T': sigma_sigma*jax.random.normal(sigma_beta_T_key, shape=(num_particles, 1))+mu_sigma}
    return key, initial_position

N = 20
D = 2
M = N*(N-1)//2
n_observations = 3
triu_indices = np.triu_indices(N,k=1)

# Initialize PRNGKey
key = jax.random.PRNGKey(0)

# Create observation
key, gt_prior = sample_prior(key, (N,D))
key, obs = sample_observation(key, gt_prior, n_observations, dirty=False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Set functions
# _bookstein_log_prior = lambda state: bookstein_log_prior(**state)
# _bookstein_log_likelihood = lambda state: bookstein_log_likelihood(**state, dirty=False, obs=obs)
# 
# # RMH parameters
# rmh_parameters = {'sigma':1e-2*jnp.eye((N-D)*D+1)}
# 
# # Create SMC object
# smc = bjx.adaptive_tempered_smc(
#     logprior_fn = _bookstein_log_prior,
#     loglikelihood_fn = _bookstein_log_likelihood,
#     mcmc_algorithm = bjx.rmh,
#     mcmc_parameters = rmh_parameters,
#     resampling_fn = resampling.systematic,
#     target_ess = 0.5,
#     mcmc_iter = 1_000,
# )
# 
# num_particles = 1_000
# 
# # Create SMC state (with Bookstein coordinates)
# init_key = jax.random.PRNGKey(5678)
# key, init_position = initialize_bkst_particles(init_key, num_particles, (N,D))
# initial_smc_state = smc.init(init_position)
# 
# # Loop-the-loop
# key, n_iter, states_rwm_smc = smc_bkst_inference_loop(key, smc.step, initial_smc_state)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Add bkst position to each position in trace
# bkst_target = get_bookstein_target(D)
# add_bkst = lambda c, s : (None, jnp.concatenate([bkst_target, states_rwm_smc.particles['_z'][s,:,:]]))
# _, bkst_z_pos = jax.lax.scan(add_bkst, None, jnp.arange(num_particles))
# states_rwm_smc.particles['_z'] = bkst_z_pos

size = 5
theta = 0.0 # Just plot everything

n_trace_samples = 5
sample_idx =  np.sort(np.random.randint(1, num_particles, size=n_trace_samples))

obs_pidx = np.random.randint(n_observations)

# Observed & prior values
gt_params = get_det_params(gt_prior['_z'])
gt_pos_poincare = lorentz_to_poincare(gt_params['z'])
gt_d_norm = triu2mat(gt_params['d_norm'])
gt_edge_weights = triu2mat(obs[obs_pidx])
obs_network = nx.from_numpy_array(gt_edge_weights)

nrows, ncols = 3,1+n_trace_samples
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*size, nrows*size))

# Plot prior network
prior_pos_dict = {j: gt_pos_poincare[j, :] for j in range(N)}
plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,0],
                    title='Ground truth network',  threshold=theta)
poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
axes[0,0].add_patch(poincare_disk)

# Plot prior distances
axes[1,0].imshow(gt_d_norm)
axes[1,0].set_title('Ground truth distances')

# Plot observed correlations
axes[2,0].imshow(gt_edge_weights)
axes[2,0].set_title('Observed correlations (sample {})'.format(obs_pidx))
axes[2,0].set_xlabel('Node')
axes[2,0].set_ylabel('Node')

for i, s in enumerate(sample_idx):
    # Get values
    state_params = get_det_params(states_rwm_smc.particles['_z'][s,:,:])
    sample_pos = lorentz_to_poincare(state_params['z'])
    sampled_d = triu2mat(state_params['d_norm'])

    # Plot sampled network
    prior_pos_dict = {j: sample_pos[j, :] for j in range(N)}
    plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,i+1],
                        title='Sample {} positions'.format(s),  threshold=theta, bkst=True)
    poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
    axes[0,i+1].add_patch(poincare_disk)

    # Plot sampled distances
    axes[1,i+1].imshow(sampled_d)
    axes[1,i+1].set_title('Sample {} distances'.format(s))

    # Turn off last axis
    axes[2,i+1].scatter(gt_params['d_norm'], state_params['d_norm'], c='k')
    axes[2,i+1].set_xlabel('Ground truth D')
    axes[2,i+1].set_ylabel('Sampled D')

plt.tight_layout()
savetitle = 'bookstein_within_SMC_samples.png'
plt.savefig(savetitle)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gt_d_triu = gt_params['d_norm']
# get_corr = lambda carry, position: (None, jnp.corrcoef(gt_d_triu, lorentz_distance(hyp_pnt(position))[triu_indices])[0,1])
# 
# _, smc_corrs = jax.lax.scan(get_corr, None, states_rwm_smc.particles['_z'])

plt.figure(figsize=(8,5))
plt.hist(smc_corrs, bins=n_bins)
plt.xlabel('Correlation')
plt.ylabel('Count')
title = 'Distance correlation histogram of all {:,} particles'.format(num_particles)
plt.title(title)
savetitle = 'SMC_bkst_Correlations_hist.png'
plt.savefig(savetitle)
plt.show()

"""## Method Comparison"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Observation settings
# N = 5
# D = 2
# M = N*(N-1)//2
# triu_indices = np.triu_indices(N,k=1)
# n_observations = 10
# key = jax.random.PRNGKey(1)
# 
# # Method parameters
# n_steps = 250_000
# burn_in = 25_000
# s = 1e-2
# bkst_target = get_bookstein_target(D)
# mcmc_iter = 1_000
# num_particles = 1_000
# 
# # Create keys for each iteration
# n_networks = 5
# n_inits = 5
# keys = jax.random.split(key, num=n_networks)
# 
# # Keilelijk but we save the corrs in lists
# rmh_corrs_lst = []
# rmh_bkst_corrs_lst = []
# smc_corrs_lst = []
# smc_bkst_corrs_lst = []
# 
# for i, key in enumerate(keys):
#     print('Network',i)
#     # Create observation
#     key, gt_prior = sample_prior(key, (N,D))
#     key, obs = sample_observation(key, gt_prior, n_samples=n_observations, dirty=False)
# 
#     # Function to get correlations
#     gt_d_triu = get_all_params(gt_prior['_z'], gt_prior['sigma_beta_T'])['d_norm']
#     get_corr = lambda c, position: (None, jnp.corrcoef(gt_d_triu, lorentz_distance(hyp_pnt(position))[triu_indices])[0,1])
#     add_bkst = lambda c, position : (None, jnp.concatenate([bkst_target, position]))
# 
#     for j in range(n_inits):
#         # RMH
#         start = time.time()
#         lambda_log_density = lambda x: log_density(**x, obs=obs, dirty=False)
#         key, rmh_prior = sample_prior(key,(N,D))
#         initvals = {'_z':rmh_prior['_z'], 'sigma_beta_T':rmh_prior['sigma_beta_T']}
#         rmh = bjx.rmh(lambda_log_density, s*jnp.eye(N*D+1))
#         key, _, rmh_trace = rmh_sampling_loop(key, rmh.step, rmh.init(initvals), n_steps)
#         _, rmh_corrs = jax.lax.scan(get_corr, None, rmh_trace['state'].position['_z'])
#         rmh_corrs_lst.append(rmh_corrs[burn_in:])
#         end = time.time()
#         print('  RMH {} time: {:.2f}sec'.format(j,end-start))
# 
#         # RMH + Bkst
#         start = time.time()
#         bkst_log_density = lambda state: bookstein_log_prior(**state, dirty=False) + bookstein_log_likelihood(**state, obs=obs, dirty=False) # Both prior and likelihood take _z, sigma in that order
#         bkst_init, bkst_step = bjx.mcmc.rmh.init, bjx.mcmc.rmh.kernel()
#         initial_state = bookstein_init(bkst_init, rmh_prior, bkst_log_density)
#         key, _, bkst_rmh_trace = rmh_bkst_sampling_loop(key, bkst_step, bkst_log_density, initial_state, {'sigma':s*jnp.eye((N-D)*D+1)}, n_steps)
#         bkst_rmh_trace['state'].position['_z'] = jax.lax.scan(add_bkst, None, bkst_rmh_trace['state'].position['_z'])[1]
#         _, rmh_bkst_corrs = jax.lax.scan(get_corr, None, bkst_rmh_trace['state'].position['_z'])
#         rmh_bkst_corrs_lst.append(rmh_bkst_corrs[burn_in:])
#         end = time.time()
#         print('  RMH + Bkst {} time: {:.2f}sec'.format(j,end-start))
# 
#         # SMC
#         start = time.time()
#         smc = bjx.adaptive_tempered_smc(
#             logprior_fn = lambda state: log_prior(**state),
#             loglikelihood_fn = lambda state: log_likelihood(**state, obs=obs, dirty=False),
#             mcmc_algorithm = bjx.rmh,
#             mcmc_parameters = {'sigma': s*jnp.eye(N*D+1)} ,
#             resampling_fn = resampling.systematic,
#             target_ess = 0.5,
#             mcmc_iter = mcmc_iter)
#         key, init_position = initialize_particles(key, num_particles, (N,D), dirty=False)
#         initial_smc_state = smc.init(init_position)
#         key, _, smc_states = smc_inference_loop(key, smc.step, initial_smc_state)
#         _, smc_corrs = jax.lax.scan(get_corr, None, smc_states.particles['_z'])
#         smc_corrs_lst.append(smc_corrs)
#         end = time.time()
#         print('  SMC {} time: {:.2f}sec'.format(j,end-start))
# 
#         # SMC + Bkst
#         start = time.time()
#         smc = bjx.adaptive_tempered_smc(
#             logprior_fn = lambda state: bookstein_log_prior(**state, dirty=False),
#             loglikelihood_fn = lambda state: bookstein_log_likelihood(**state, obs=obs, dirty=False),
#             mcmc_algorithm = bjx.rmh,
#             mcmc_parameters = {'sigma':1e-2*jnp.eye((N-D)*D+1)},
#             resampling_fn = resampling.systematic,
#             target_ess = 0.5,
#             mcmc_iter = mcmc_iter)
#         key, init_position = initialize_bkst_particles(key, num_particles, (N,D), dirty=False)
#         initial_smc_state = smc.init(init_position)
#         key, _, smc_bkst_states = smc_bkst_inference_loop(key, smc.step, initial_smc_state)
#         smc_bkst_states.particles['_z'] = jax.lax.scan(add_bkst, None, smc_bkst_states.particles['_z'])[1]
#         _, smc_bkst_corrs = jax.lax.scan(get_corr, None, smc_bkst_states.particles['_z'])
#         smc_bkst_corrs_lst.append(smc_bkst_corrs)
#         end = time.time()
#         print('  SMC + Bkst {} time: {:.2f}sec'.format(j,end-start))

n_bins = 100

hist_matrix = np.zeros((4, n_networks, n_inits, n_bins))
bins_matrix = np.zeros((4, n_networks, n_inits, n_bins+1))
for i in range(n_networks):
    for j in range(n_inits):
        rmh_hist, rmh_bins = np.histogram(rmh_corrs_lst[i*n_inits+j], bins=n_bins)
        hist_matrix[0,i,j,:] = rmh_hist
        bins_matrix[0,i,j,:] = rmh_bins

        rmh_bkst_hist, rmh_bkst_bins = np.histogram(rmh_bkst_corrs_lst[i*n_inits+j], bins=n_bins)
        hist_matrix[1,i,j,:] = rmh_bkst_hist
        bins_matrix[1,i,j,:] = rmh_bkst_bins

        smc_hist, smc_bins = np.histogram(smc_corrs_lst[i*n_inits+j], bins=n_bins)
        hist_matrix[2,i,j,:] = smc_hist
        bins_matrix[2,i,j,:] = smc_bins

        smc_bkst_hist, smc_bkst_bins = np.histogram(smc_bkst_corrs_lst[i*n_inits+j], bins=n_bins)
        hist_matrix[3,i,j,:] = smc_bkst_hist
        bins_matrix[3,i,j,:] = smc_bkst_bins

filename = 'ConHyp_corrs_histogram.pkl'

corrs_histogram = {'hist':hist_matrix, 'bins':bins_matrix}
with open(filename, 'wb') as f:
    pickle.dump(corrs_histogram, f)

files.download(filename)

with open(filename, 'rb') as f:
    loaded_dict = pickle.load(f)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# size = 5
# nrows, ncols = 2,2
# fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*size, nrows*size))
# colors = list(mcolors.TABLEAU_COLORS.keys())
# alpha = .7
# 
# for i in range(n_networks):
#     label = 'Network {}'.format(i)
#     for j in range(n_inits):
#         # RMH
#         if j == 0:
#             axes[0,0].stairs(hist_matrix[0,i,j,:], bins_matrix[0,i,j,:], fill=True, label=label, color=colors[i], alpha=alpha)
#         else:
#             axes[0,0].stairs(hist_matrix[0,i,j,:], bins_matrix[0,i,j,:], fill=True, color=colors[i], alpha=alpha)
#         # axes[0,0].set_xlabel('Correlation')
#         axes[0,0].set_ylabel('Count')
#         axes[0,0].set_title('RMH')
# 
#         # RMH + Bkst
#         axes[0,1].stairs(hist_matrix[1,i,j,:], bins_matrix[1,i,j,:], fill=True, color=colors[i], alpha=alpha)
#         # axes[0,1].set_xlabel('Correlation')
#         # axes[0,1].set_ylabel('Count')
#         axes[0,1].set_title('RMH+Bookstein')
# 
#         # SMC
#         axes[1,0].stairs(hist_matrix[2,i,j,:], bins_matrix[2,i,j,:], fill=True, color=colors[i], alpha=alpha)
#         axes[1,0].set_xlabel('Correlation')
#         axes[1,0].set_ylabel('Count')
#         axes[1,0].set_title('SMC')
# 
#         # SMC + Bkst
#         axes[1,1].stairs(hist_matrix[3,i,j,:], bins_matrix[3,i,j,:], fill=True, color=colors[i], alpha=alpha)
#         axes[1,1].set_xlabel('Correlation')
#         # axes[1,1].set_ylabel('Count')
#         axes[1,1].set_title('SMC+Bookstein')
# 
# xlims_lst = [axes[i,j].get_xlim() for i in range(2) for j in range(2)]
# xmin, xmax = min([xlims_lst[i][0] for i in range(4)]), max([xlims_lst[i][1] for i in range(4)])
# 
# for i in range(2):
#     for j in range(2):
#         axes[i,j].set_xlim(xmin, xmax)
# 
# # Invisible subplot for big shared axes.
# fig.add_subplot(111, frameon=False)
# handles, labels = axes[0,0].get_legend_handles_labels()
# fig.legend(handles, labels, loc='right')
# plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)
# plt.xlabel('Without vs. with Bookstein', fontsize=18, labelpad=20)
# plt.ylabel('RMH vs. SMC', fontsize=18, labelpad=20)
# 
# plt.show()

"""## SVM Test"""

N = 20
D = 2
M = N*(N-1)//2
triu_indices = np.triu_indices(N,k=1)

n_tasks = 7
n_subjects = 15 # Number of embeddings per class
n_obs = 2 # Number of observations per embedding (LR|RL)
n_test = 15 # Number of embeddings in the test set
T = n_tasks*n_subjects # Total number of embeddings
assert T%n_test == 0, "T={} is not divisible by n_test={}".format(T,n_test)
n_iter = T//n_test # Number of cross validations embeddings possible

# Initialize PRNGKey
key = jax.random.PRNGKey(2)

# Create training and testing data
ground_truths = {}
data = jnp.zeros((T, n_obs, M))
true_class_labels = []
for i_task in range(n_tasks):
    # Sample prior
    key, gt_prior = sample_prior(key, (N,D))
    ground_truths[i_task] = gt_prior

    # Sample data
    for i_emb in range(n_subjects):
        key, obs_corrs = sample_observation(key, gt_prior, n_samples=n_obs, dirty=False)
        data = data.at[i_task*n_subjects+i_emb].set(obs_corrs)
        true_class_labels.append(i_task)
true_class_labels = jnp.array(true_class_labels)

_z_permutation_std = 0.2
sigma_permutation_std = 0.2

# Create training and testing data
ground_truths = {}
data = jnp.zeros((T, n_obs, M))
true_class_labels = []

# Sample prior
key, base_prior = sample_prior(key, (N,D), sigma_sigma=0.3) # We want a low sigma, so that the observations are a bit more trustworthy.
for i_task in range(n_tasks):
    # Add jitter to base prior
    key, _z_key, sigma_key = jax.random.split(key, num=3)
    _z_offset = _z_permutation_std*jax.random.normal(_z_key, shape=(N,D))
    sigma_offset = sigma_permutation_std*jax.random.normal(sigma_key, shape=(M,))

    gt_prior['_z'] += _z_offset
    gt_prior['sigma_beta_T'] += sigma_offset

    ground_truths[i_task] = gt_prior

    # Sample data
    for i_emb in range(n_subjects):
        key, obs_corrs = sample_observation(key, gt_prior, n_samples=n_obs, dirty=False)
        data = data.at[i_task*n_subjects+i_emb].set(obs_corrs)
        true_class_labels.append(i_task)
true_class_labels = jnp.array(true_class_labels)

stepsize = 1e-2*jnp.eye(N*D+1)
mcmc_iter = 200
n_particles = 1_000

LSM_embeddings = jnp.zeros((T,n_particles,N,D))
LSM_sigmas = jnp.zeros((T,n_particles,M))

bkst_target = get_bookstein_target(D)
add_bkst = lambda c, position : (None, jnp.concatenate([bkst_target, position]))

# Embed all networks.
for i_network in range(T):
    start = time.time()

    smc = bjx.adaptive_tempered_smc(
        logprior_fn = lambda state: bookstein_log_prior(**state),
        loglikelihood_fn = lambda state: bookstein_log_likelihood(**state, dirty=False, obs=data[i_network,:,:]),
        mcmc_algorithm = bjx.rmh,
        mcmc_parameters = {'sigma':1e-2*jnp.eye((N-D)*D+1)},
        resampling_fn = resampling.systematic,
        target_ess = 0.5,
        mcmc_iter = mcmc_iter,
    )
    key, init_position = initialize_bkst_particles(key, n_particles, (N,D), dirty=False)
    key, _, smc_bkst_states = smc_bkst_inference_loop(key, smc.step, smc.init(init_position))
    smc_bkst_states.particles['_z'] = jax.lax.scan(add_bkst, None, smc_bkst_states.particles['_z'])[1]

    LSM_embeddings = LSM_embeddings.at[i_network,:,:,:].set(smc_bkst_states.particles['_z']) # It lines up so nicely ;)
    LSM_sigmas = LSM_sigmas.at[i_network,:,:].set(smc_bkst_states.particles['sigma_beta_T'])
    end = time.time()
    print('Embedding {}/{} took {:.1f}sec'.format(i_network+1, T, end-start))

_z_permutation_std = .2
sigma_permutation_std = .2

filename = 'LSM_embeddings_SVM_hard_mode_z={:.1f}_sigma={:.1f}.pkl'.format(_z_permutation_std, sigma_permutation_std)

stats = {'N':N, 'D':D, 'n_tasks':n_tasks, 'n_subjects':n_subjects, 'n_obs':n_obs, 'n_particles'=n_particles}
trained_LSMs = {'_z':LSM_embeddings, 'sigma_beta_T':LSM_sigmas, 'stats':stats, 'true_class_labels':true_class_labels}
with open(filename, 'wb') as f:
    pickle.dump(trained_LSMs, f)

files.download(filename)

with open(filename, 'rb') as f:
    trained_LSMs = pickle.load(f)

LSM_embeddings = trained_LSMs['_z']
LSM_sigmas = trained_LSMs['sigma_beta_T']
stats = trained_LSMs['stats']
true_class_labels = trained_LSMs['true_class_labels']
N, D, n_tasks, n_subjects, n_obs, n_particles = stats['N'], stats['D'], stats['n_tasks'], stats['n_subjects'], stats['n_obs'], stats['n_particles']

M = N*(N-1)//2
triu_indices = np.triu_indices(N,k=1)

n_test = 15
T = n_tasks*n_subjects
assert T%n_test == 0, "T={} is not divisible by n_test={}".format(T,n_test)
n_iter = T//n_test

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Take the mean distance over the whole trace (point estimate)
# LSM_distances_avg = jnp.array([get_det_params(jnp.mean(LSM_embeddings,axis=1)[i,:,:], dirty=False)['d_norm'] for i in range(T)])

# Shuffle the data and class labels in the same order. Is not even trying to balance classes, but I guess already works.
key, subkey = jax.random.split(key)
shuffle_idx = jax.random.permutation(subkey, jnp.arange(T), independent=True)
shuffle_class_labels = true_class_labels[shuffle_idx]
shuffle_LSM_distances_avg = LSM_distances_avg[shuffle_idx]

correct = 0

## Cross-validation ##
for k in range(n_iter):
    # Divide embeddings into train/test sets
    test_incl = jnp.repeat(False,T)
    test_incl = test_incl.at[k*n_test:(k+1)*n_test].set(True)
    train_incl = jnp.logical_not(test_incl)

    train_set = shuffle_LSM_distances_avg[train_incl,:]
    test_set = shuffle_LSM_distances_avg[test_incl,:]

    train_class_labels = shuffle_class_labels[train_incl]
    test_class_labels = shuffle_class_labels[test_incl]

    # Train SVM on train set
    clf = svm.SVC()
    clf.fit(train_set, train_class_labels)

    # Predict class labels on test set
    guessed_labels = clf.predict(test_set)

    correct_iter = jnp.sum(guessed_labels == test_class_labels)
    print('      Correct: {}/{}'.format(correct_iter,n_test))
    correct += correct_iter
print('Total Correct: {}/{} = {:.2f}%'.format(correct,T, correct/T*100))

def get_attribute_from_trace(LSM_embeddings:ArrayLike, attribute:str='d_norm', shape:tuple=None) -> jnp.ndarray:
    """
    Calculates the distance for a whole trace of positions
    PARAMS:
    LSM_embeddings : (T, n_particles, N, D) positions for all traces of all embeddings
    """
    T, n_particles, N, _ = LSM_embeddings.shape
    if shape is None:
        M = N*(N-1)//2
        shape = (T, n_particles, M)

    def cond(carry):
        k, _, _ = carry
        return k < T

    @jax.jit
    def step(carry):
        k, i, attributes = carry
        attributes = attributes.at[k,i,:].set(get_det_params(LSM_embeddings[k,i,:,:], dirty=False)[attribute])
        i = (i+1)%n_particles
        next_k = i == 0
        k = k+next_k
        return k, i, attributes

    _, _, attributes = jax.lax.while_loop(
        cond, step, (0, 0, jnp.zeros(shape))
        )
    return attributes

LSM_distance_trace = get_attribute_from_trace(LSM_embeddings, 'd_norm')

# Shuffle the data and class labels in the same order. Is not even trying to balance classes, but I guess already works.
key, subkey = jax.random.split(key)
shuffle_idx = jax.random.permutation(subkey, jnp.arange(T), independent=True)
shuffle_class_labels = true_class_labels[shuffle_idx]
shuffle_LSM_distance_trace = LSM_distance_trace[shuffle_idx]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# accuracies = np.zeros((n_iter*n_particles))
# 
# ## Cross-validation ##
# for k in range(n_iter):
#     # Divide embeddings into train/test sets
#     test_incl = jnp.repeat(False,T)
#     test_incl = test_incl.at[k*n_test:(k+1)*n_test].set(True)
#     train_incl = jnp.logical_not(test_incl)
# 
#     train_set = shuffle_LSM_distance_trace[train_incl,:,:]
#     test_set = shuffle_LSM_distance_trace[test_incl,:,:]
# 
#     train_class_labels = shuffle_class_labels[train_incl]
#     test_class_labels = shuffle_class_labels[test_incl]
# 
#     for n in range(n_particles):
#         # Train SVM on train set
#         clfs = svm.SVC()
#         clf.fit(train_set[:,n,:], train_class_labels)
#         # Predict class labels on test set
#         guessed_labels = clf.predict(test_set[:,n,:])
#         accuracies[k*n_particles+n] = np.sum(guessed_labels == test_class_labels)/n_test

margin=0.05
n_bins = n_iter

plt.figure(figsize=(6,6))
acc_hist, acc_bins = jnp.histogram(accuracies, bins=n_bins)
plt.stairs(acc_hist, acc_bins, fill=True)
plt.xlabel('accuracy')
plt.ylabel('probability')
plt.xlim((-margin,1+margin))
plt.show()

LSM_z_trace = get_attribute_from_trace(LSM_embeddings, attribute='z', shape=(T, n_particles, N, D+1))

def plot_posterior(pos_trace:ArrayLike,
                   edge_trace:ArrayLike=None,
                   ax:Axes=None,
                   title:str=None,
                   edge_width:float=0.5,
                   disk_radius:float=1.,
                   hyperbolic:bool=False,
                   continuous:bool=False,
                   bkst:bool=False,
                   legend:bool=False,
                   threshold:float=0.4,
                   margin:float=0.1,
                   s:float=0.5,
                   alpha_margin:float=0.005) -> Axes:
    """
    Plots a network with the given positions.
    PARAMS:
    pos_trace : (n_steps, N, D+1) trace of the positions
    edge_trace : (n_steps, M) trace of the std of the edges. Must first be determined from sigma_beta_T (continuous) or T (binary) parameter.
    ax : axis to plot the network in
    title : title to display
    edge_width : width of the edges
    disk_radius : the radius of the size of the plot
    hyperbolic : whether the network should be plotted in hyperbolic space or Euclidean space
    continuous : whether the edge weights are continuous or binary
    bkst : whether to deal with the first two nodes as Bookstein nodes
    threshold : minimum edge weight for the edge to be plotted
    margin : percentage of disk radius to be added as whitespace
    s : point size for the scatter plot
    alpha_margin : transparancy margin to ensure the most variable position does not have alpha=0.
    """
    n_steps, N, D = pos_trace.shape
    assert (D==2) or (D==3 and hyperbolic), 'Dimension must be 2 to be plotted, or 3 pre-hyperbolic projection'
    clean_ax = True
    if ax is None:
        plt.figure(facecolor='w')
        ax = plt.gca()
        clean_ax = False

    # Convert hyperbolic Lorentz positions to plottable Poincaré coordinates
    if hyperbolic:
        pos_trace = lorentz_to_poincare(pos_trace)

    pos_mean = np.mean(pos_trace, axis=0) # N,D average position
    pos_std = np.std(pos_trace, axis=0) # N,D position standard deviation
    pos_std_nml = (pos_std)/np.max(pos_std+alpha_margin) # Normalize so that the max standard deviation is (just under) 1 (which then corresponds to most transparant point)

    # Plot node means
    if bkst: # Make bookstein coordinates red
        ax.scatter(pos_mean[:2,0], pos_mean[:2,1], c='r', s=s, label='Bookstein coordinates')
        ax.scatter(pos_mean[2:,0], pos_mean[2:,1], c='k', s=s)
        if legend:
            ax.legend(loc='best', bbox_to_anchor=(1.01, 0.5))
    else:
        ax.scatter(pos_mean[:,0], pos_mean[:,1], c='k', s=s)

    # Plot standard deviations
    for n in range(N):
        std = float(np.max(pos_std_nml[n,:]))
        point = Ellipse((pos_mean[n,0], pos_mean[n,1]), width=pos_std[n,0], height=pos_std[n,1], alpha=1-std, color='r', fill=True)
        ax.add_patch(point)

    if title is not None:
        ax.set_title(title, color='k', fontsize='24')
    margin = 1+margin
    ax.set(xlim=(-margin*disk_radius,margin*disk_radius),ylim=(-margin*disk_radius,margin*disk_radius))
    ax.axis('off')
    if not clean_ax:
        plt.tight_layout()
    return ax

plt.figure(figsize=(6,6))
ax = plt.gca()
plot_posterior(LSM_z_trace[0], ax=ax, hyperbolic=True, bkst=True, s=.5)
poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
ax.add_patch(poincare_disk)
plt.show()

size = 5
theta = 0.0 # Just plot everything

n_trace_samples = 5
sample_idx =  np.sort(np.random.randint(1, num_particles, size=n_trace_samples))

obs_pidx = np.random.randint(n_observations)

# Observed & prior values
gt_params = get_det_params(gt_prior['_z'])
gt_pos_poincare = lorentz_to_poincare(gt_params['z'])
gt_d_norm = triu2mat(gt_params['d_norm'])
gt_edge_weights = triu2mat(obs[obs_pidx])
obs_network = nx.from_numpy_array(gt_edge_weights)

nrows, ncols = 3,1+n_trace_samples
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*size, nrows*size))

# Plot prior network
prior_pos_dict = {j: gt_pos_poincare[j, :] for j in range(N)}
plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,0],
                    title='Ground truth network',  threshold=theta)
poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
axes[0,0].add_patch(poincare_disk)

# Plot prior distances
axes[1,0].imshow(gt_d_norm)
axes[1,0].set_title('Ground truth distances')

# Plot observed correlations
axes[2,0].imshow(gt_edge_weights)
axes[2,0].set_title('Observed correlations (sample {})'.format(obs_pidx))
axes[2,0].set_xlabel('Node')
axes[2,0].set_ylabel('Node')

for i, s in enumerate(sample_idx):
    # Get values
    state_params = get_det_params(states_rwm_smc.particles['_z'][s,:,:])
    sample_pos = lorentz_to_poincare(state_params['z'])
    sampled_d = triu2mat(state_params['d_norm'])

    # Plot sampled network
    prior_pos_dict = {j: sample_pos[j, :] for j in range(N)}
    plot_network(obs_network, hyperbolic=True, continuous=True, pos=prior_pos_dict, ax=axes[0,i+1],
                        title='Sample {} positions'.format(s),  threshold=theta, bkst=True)
    poincare_disk = plt.Circle((0, 0), 1, color='k', fill=False, clip_on=False)
    axes[0,i+1].add_patch(poincare_disk)

    # Plot sampled distances
    axes[1,i+1].imshow(sampled_d)
    axes[1,i+1].set_title('Sample {} distances'.format(s))

    # Turn off last axis
    axes[2,i+1].scatter(gt_params['d_norm'], state_params['d_norm'], c='k')
    axes[2,i+1].set_xlabel('Ground truth D')
    axes[2,i+1].set_ylabel('Sampled D')

plt.tight_layout()
savetitle = 'bookstein_within_SMC_samples.png'
plt.savefig(savetitle)
plt.show()